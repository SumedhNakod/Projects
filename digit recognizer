import lasagne
import theano
import theano.tensor as T
def build_NN(input_var=None):
  l_in=lasagne.layers.InputLayer(shape=(None,1,28,28),input_var=input_var)
  #drop 20% edges to prevent overfitting
  l_in_drop=lasagne.layers.DropoutLayer(l_in,p=0.2)
  #hidden layers
  l_hid1=lasagne.layers.DenseLayer(l_in_drop,num_units=800,
                                    nonlinearity=lasagne.nonlinearities.rectify,
                                    W=lasagne.init.GlorotUniform())#initializes weights randomly
  #drop 50% from hidden layer
  l_hid1_drop=lasagne.layers.DropoutLayer(l_hid1,p=0.5)
  #hidden layers
  l_hid2 =lasagne.layers.DenseLayer(l_hid1_drop,num_units=800,
                                    nonlinearity=lasagne.nonlinearities.rectify,
                                    W=lasagne.init.GlorotUniform())
  #drop 50% from hidden layer 2
  l_hid2_drop=lasagne.layers.DropoutLayer(l_hid2,p=0.5)
  
  #Output Layer
  l_out = lasagne.layers.DenseLayer(l_hid2_drop,num_units=10,
                                    nonlinearity=lasagne.nonlinearities.softmax)#specifies each output is between 0-1 and max of those will be the predicted digit
  return l_out

#training network
input_var=T.tensor4('inputs') #empty 4D array
target_var=T.ivector('targets') #an empty 1D int array

network = build_NN(input_var)

#error function
prediction = lasagne.layers.get_output(network)
loss = lasagne.objectives.categorical_crossentropy(prediction,target_var)#difference between original and predicted value
loss = loss.mean()

params = lasagne.layers.get_all_params(network) #current weights
updates = lasagne.updates.nesterov_momentum(loss,params,
                                            learning_rate=0.01,
                                            momentum = 0.9)

train_fn= theano.function([input_var , target_var],loss,updates=updates)
